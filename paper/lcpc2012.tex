
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{lineno}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\urldef{\mails}\path|{firstname.lastname}@epfl.ch|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\makeatletter
\newcommand{\customlabel}[2]{%
\protected@write \@auxout {}{\string \newlabel {#1}{{#2}{}}}}
\makeatother

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{FlowPools: Lock-Free Deterministic Concurrent Data-Flow Queues}

% a short form should be given in case it is too long for the running head
\titlerunning{FlowPools: Lock-Free Deterministic Concurrent Data-Flow Queues}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Authors}
%
% \author{Alfred Hofmann%
% \thanks{Please note that the LNCS Editorial assumes that all authors have used
% the western naming convention, with given names preceding surnames. This determines
% the structure of the names in the running heads and the author index.}%
% \and Ursula Barth\and Ingrid Haas\and Frank Holzwarth\and\\
% Anna Kramer\and Leonie Kunz\and Christine Rei\ss\and\\
% Nicole Sator\and Erika Siebert-Cole\and Peter Stra\ss er}

\authorrunning{FlowPools: Lock-Free Deterministic Concurrent Data-Flow Queues}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{EPFL, Switzerland\\
\mails\\
\url{http://lamp.epfl.ch}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

% \toctitle{Lecture Notes in Computer Science}
% \tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
Implementing correct and deterministic parallel programs is hard (additional motivation sentence after this). We present the design and implementation of a fundamental data structure for deterministic parallel data-flow computation. Aditionally, we provide a proof of correctness, showing that the implementation is linearizable, lock-free, and deterministic. Finally, we provide microbenchmarks which compare our \emph{flow pools} against corresponding operations on other popular concurrent data structures, in addition to performance benchmarks on a real \emph{XYZ} application using real data. (Keep abstract between 70 and 150 words).
\keywords{data-flow, concurrent data-structure, determinism}
\end{abstract}


\section{Introduction}


\subsection{Motivation}

- we want a deterministic model
- we do not block in the programming model (i.e. there are no
operations which cause blocking until a value becomes available)
- we want a non-blocking data-structure (i.e. the operations on the
data-structures should themselves be non-blocking)
- programs run indefinitely => we need to GC parts of the
data structure we no longer need
- we want to reduce heap allocation and inline the datastructure as
much as possible => lower memory consumption, better cache behaviour
and fewer GC cycles


Obligatory multicore motivation paragraph.

Lock-free is better, and why.

Introduction and motivation for data-flow programming model.

\section{Model of Computation}
Producer-consumer parallelism. Description and image of queue/stream of values, producer, and multiple consumers. 

\section{Programming Model}
The FlowPool suite supports the following operations:
\begin{itemize}
\item \texttt{Builder.<<(x: T): Builder}\\
  Inserts an element into the underlying FlowPool.
\item \texttt{Builder.seal(n: Int): Unit}\\
  Seals the underlying FlowPool at \texttt{n} elements. The need for
  the size argument is explained below. A sealed FlowPool may only
  contain \texttt{n} elements. This allows for callback cleanup and
  termination.
\item \texttt{FlowPool.doForAll(f: T => Unit): Future[Int]}\\
  Instructs
  the FlowPool to execute the closure \texttt{f} exactly once for each
  element inserted into the FlowPool (asynchronously). The returned
  future contains the number of elements in the FlowPool and completes
  once \texttt{f} has been executed for all elements.
\item \texttt{FlowPool.mappedFold[U, V <: U](acc: V)(cmb: (U,V) =>
    V)(map: T => U): Future[(Int, V)]}
  Reduces the FlowPool to a single value of type V, by first mapping
  each element to an internal representation using \texttt{map} and
  then aggregating using \texttt{cmb}. No guarantee is given about
  synchronization or order. Returns a future containing a tuple with
  number of elements in the FlowPool and the aggregated value.
\item \texttt{FlowPool.builder}\\
  Returns a builder for this FlowPool.
\item \texttt{Future.map[U](f: T => U)}\\
  Maps this future to another
  future executing the function \texttt{f} exactly once when the first
  future completes.
\item \texttt{future[T](f: () => T): Future[T]}\\
  Asynchronously dispatch execution of \texttt{f} an return a future
  with its result.
\end{itemize}

\paragraph{Determinism of \texttt{seal}} We will show that the final
size of the FlowPool is required as an argument to the seal method in
order to satisfy the determinism property of the FlowPool. Look at the
following program:

\begin{verbatim}
val p = new FlowPool[Int]()
val b = p.builder

future {
  for (i <- 1 to 10) { b << i }
  b.seal
}

future { for (i <- 1 to 10) { b << i } }
\end{verbatim}

Depending on which for-loop completes first, this program completes
successfully or yields an error. A similar program with
\verb+b.seal(20)+ will always succeed.

\paragraph{Generators} In the following we'll present a couple of
generators for FlowPools based on common generators in the Scala
standard library.

\begin{verbatim}
def iterate[T](start: T, len: Int)(f: (T) => T) = {
  val p = new FlowPool[T]
  val b = p.builder
  future {
    var e = start
    for (i <- 1 to len) { b << e; e = f(e) }
    b.seal(len)
  }; p
}
\end{verbatim}

\begin{verbatim}
def tabulate[T](n: Int)(f: (Int) => T) = {
  val p = new FlowPool[T]
  val b = p.builder
  future {
    for (i <- 0 to (n-1)) {
      b << f(i)
    }
    b.seal(n)
  }; p
}
\end{verbatim}

\begin{verbatim}
def fill[T](n: Int)(elem: => T) = {
  val p = new FlowPool[T]
  val b = p.builder
  future {
    for (i <- 1 to n) { b << elem }
    b.seal(n)
  }; p
}
\end{verbatim}

\paragraph{Monadic Operations} In the following we'll present some
monadic operations on top of the basic FlowPool operations. This will
also show some use-cases of the futures as result type of
\verb+doForAll+.

\begin{verbatim}
def map[S](f: T => S) = {
  val fp = new FlowPool[S]
  val b  = fp.builder
  doForAll { x =>
    b << f(x)
  } map { b.seal _ }
  fp
}
\end{verbatim}

\begin{verbatim}
def filter(f: T => Boolean) = {
  val fp = new FlowPool[T]
  val b  = fp.builder

  mappedFold(0)(_ + _) { x =>
    if (f(x)) { b << x; 1 } else 0
  } map { case (c,fc) => b.seal(fc) }

  fp
}
\end{verbatim}

\begin{verbatim}
def flatMap[S](f: T => FlowPool[S]) = {
  val fp = new FlowPool[S]
  val b  = fp.builder

  mappedFold(future(0))(_ <+> _) { x =>
    f(x).doForAll(b << _)
  } map { case (c,cfut) => cfut.map(b.seal _) }

  fp
}
\end{verbatim}
where \verb|<+>| is the future of the sum of two \verb+Future[Int]+.

\section{Implementation}

\begin{alltt}
{\scriptsize
type Elem

type Block \{
  array: Array[Elem]
  next: Block
  index: Int
  blockindex: Int
\}

type FlowPool \{
  start: Block
  current: Block
\}

type Terminal \{
  sealed: Int
  callbacks: List[Elem => Unit]
\}

BLOCKSIZE = 256
LASTELEMPOS = BLOCKSIZE - 2
NOTSEALED = -1
}
\end{alltt}


\setlength\linenumbersep{2pt}


\begin{alltt}
{\scriptsize
{\internallinenumbers{def create()
  new FlowPool \{
    start = createBlock(0)
    current = start
  \}

def createBlock(bidx: Int)
  new Block \{
    array = new Array(BLOCKSIZE)
    index = 0
    blockindex = bidx
    next = null
  \}

def append(elem: Elem)
  b = READ(current) {\customlabel{read_block}{\LineNumber}}
  idx = READ(b.index) {\customlabel{read_index}{\LineNumber}}
  nextobj = READ(b.array(idx + 1)) {\customlabel{read_next}{\LineNumber}}
  curobj = READ(b.array(idx)) {\customlabel{read_current}{\LineNumber}}
  if (check(b, idx, curobj)) \{
    if (CAS(b.array(idx + 1), nextobj, curobj)) \{ {\customlabel{cas_propagate}{\LineNumber}}
      if (CAS(b.array(idx), curobj, elem)) \{ {\customlabel{cas_append}{\LineNumber}}
        WRITE(b.index, idx + 1) {\customlabel{write_append}{\LineNumber}}
        invokeCallbacks(elem, curobj)
      \} else append(elem)
    \} else append(elem)
  \} else \{
    advance()
    append(elem)
  \}

def check(b: Block, idx: Int, curobj: Object)
  if (idx > LASTELEMPOS) return false
  else curobj match \{
    elem: Elem =>
      return false
    term: Terminal =>
      if (term.sealed == NOTSEALED) return true
      else \{
        if (totalElems(b, idx) < term.sealed) return true
        else error("sealed")
      \}
    null =>
      error("unreachable")
  \}

def advance()
  b = READ(current)
  idx = READ(b.index)
  if (idx > LASTELEMPOS) expand(b, b.array(idx))
  else \{
    obj = READ(b.array(idx))
    if (obj is Elem) WRITE(b.index, idx + 1) {\customlabel{write_advance}{\LineNumber}}
  \}

def expand(b: Block, t: Terminal)
  nb = READ(b.next)
  if (nb is null) \{
    nb = createBlock(b.blockindex + 1)
    nb.array(0) = t
    if (CAS(b.next, null, nb)) {\customlabel{cas_expand}{\LineNumber}}
      expand(b, t)
  \} else \{
    CAS(current, b, nb) {\customlabel{cas_block}{\LineNumber}}
  \}

def totalElems(b: Block, idx: Int)
  return b.blockindex * (BLOCKSIZE - 1) + idx

def invokeCallbacks(elem: Elem, term: Terminal)
  for (f <- term.callbacks) future \{
    f(elem)
  \}

def seal(size: Int)
  b = READ(current)
  idx = READ(b.index)
  if (idx <= LASTELEMPOS) \{
    curobj = READ(b.array(idx)) {\customlabel{read_seal}{\LineNumber}}
    curobj match \{
      term: Terminal =>
        if tryWriteSeal(term, b, idx, size)
      elem: Elem =>
        WRITE(b.index, idx + 1) {\customlabel{write_seal}{\LineNumber}}
        seal(size)
      null =>
        error("unreachable")
    \}
  \} else \{
    expand(b, b.array(idx))
    seal(size)
  \}

def tryWriteSeal(term: Terminal, b: Block, idx: Int, size: Int)
  val total = totalElems(b, idx)
  if (total > size) error("too many elements")
  if (term.sealed == NOTSEALED) \{
    nterm = new Terminal \{
      sealed = size
      callbacks = term.callbacks
    \}
    return CAS(b.array(idx), term, nterm) {\customlabel{cas_seal}{\LineNumber}}
  \} else if (term.sealed != size) \{
    error("already sealed with different size")
  \} else return true

def doForAll(f: Elem => Unit)
  future \{
    asyncDoForAll(f, start, 0)
  \}

def asyncDoForAll(f: Elem => Unit, b: Block, idx: Int)
  if (idx <= LASTELEMPOS) \{
    obj = READ(b.array(idx)) {\customlabel{read_callback}{\LineNumber}}
    obj match \{
      term: Terminal =>
        nterm = new Terminal \{
          sealed = term.sealed
          callbacks = f \(\cup\) term.callbacks
        \}
        if (!CAS(b.array(idx), term, nterm))
          asyncDoForAll(f, b, idx) {\customlabel{cas_callback}{\LineNumber}}
      elem: Elem =>
        f(elem) {\customlabel{call_callback}{\LineNumber}}
        asyncDoForAll(f, b, idx + 1)
      null =>
        error("unreachable")
    \}
  \} else \{
    expand(b, b.array(idx))
    asyncDoForAll(f, b.next, 0)
  \}

}}}
\end{alltt}

We now describe the FlowPool and its basic operations.
In doing so, we omit the details not relevant to the algorithm
(specifically, the builder abstraction) and
focus on a high-level description of a non-blocking data structure.
One straightforward way to implement a growing pool is to use a linked
list of nodes that wrap elements.
As we are concerned about the memory footprint and cache-locality, we
store the elements into arrays instead, which we call blocks.
Whenever a block becomes full, a new block is allocated and the
previous block is made to point to the \verb=next= block.
This way, most writes amount to a simple array-write, while allocation
occurs only occasionally.
Each block contains a hint \verb=index= to the first free entry in
the array, i.e. one that does not contain an element.
An \verb=index= is a hint, since it may actually reference an earlier index.
The FlowPool maintains a reference to the first block called
\verb=start=.
It also maintains a hint to the last block in the chain of blocks,
called \verb=current=.
This reference may not always be up to date, but it always points
to some block in the chain.

Each FlowPool is associated with a list of callbacks which have
to be called in the future as new elements are added.
Each FlowPool can be in a sealed state, meaning there is a bound on
the number of elements it stores.
This information is stored as a \verb=Terminal= value in the first
free entry of the array.
At all times we maintain the invariant that the array in each block
starts with a sequence of elements, followed by a \verb=Terminal=
delimiter. From a higher-level perspective, appending an element
starts by copying the \verb=Terminal= value to the next entry and then
overwriting the current entry with the element being appended.

The \verb=append= operation starts by reading the \verb=current= block
and the \verb=index= of the free position.
It then reads the
\verb=nextobj= after the first free entry, followed by a read of the
\verb=curobj= at the free entry.
The \verb=check= procedure checks the bounds conditions, whether the
FlowPool was already sealed or if the current array entry contains an
element.
In either of these events, the \verb=current= and \verb=index= values
need to be set -- this is done in the \verb=advance= procedure.
We call this the \textbf{slow path} of the \verb=append= method.
Notice that there are several causes that trigger the slow path.
If some other thread completes the \verb=append= method but is
preempted before updating the value of the hint \verb=index=, then the
\verb=curobj= will have the type \verb=Elem=.
The same happens if a preempted thread updates the value of the
hint \verb=index= after additional elements have been added,
via unconditional write in line \ref{write_append}.
Finally, reaching an end of block triggers the slow path.

Otherwise, the operation executes the \textbf{fast path} and appends
an element.
It first copies the \verb=Terminal= value to the next entry with a CAS
instruction in line \ref{cas_propagate}, with \verb=nextobj= being the
expected value. If it fails (e.g. due to a concurrent CAS), the append
operation is restarted.
Otherwise, it proceeds by writing the element to the current entry with
a CAS in line \ref{cas_append}, the expected value being
\verb=curobj=.
If it succeeds, it updates the \verb=b.index= value and invokes all
the callbacks (present at the point when the elements was added) with
the \verb=future= construct.

Interestingly, inverting the order of the reads in lines
\ref{read_next} and \ref{read_current} would cause a race in which a
thread could overwrite a \verb=Terminal= value with some older
\verb=Terminal= value if some other thread appended an element in between.

The \verb=seal= operation continuously increases the \verb=index= in the block
until it finds the first free entry. It then tries to replace the \verb=Terminal=
value there with a new \verb=Terminal= value which has the seal size set.
An error occurs if a different seal size is set already. The \verb=foreach=
operation works in a similar way, but is executed asynchronously. Unlike
\verb=seal=, it starts from the first element in the pool and calls the
callback for each element until it finds the first free entry.
It then replaces the \verb=Terminal= value with a new \verb=Terminal=
value with the additional callback. From that point on the \verb=append=
method is responsible for scheduling that callback for subsequently added elements.
Note that all three operations call \verb=expand= to add an additional
block once the current block is empty, to ensure lock-freedom.


\section{Proofs}
\subsection{Abstract Pool Semantics}
Alex
\subsection{Linearizability}
Alex
\subsection{Lock-Freedom}

\subsection{Determinism}

\section{Experimental Results}

\section{Related Work}
Things to probably cite: Oz, gpars, Java CLQ, our futures writeup.

Things we should probably have a look at: Microsoft TPL, Dataflow
Java, FlumeJava...

Forcing a bib, \cite{bowman:reasoning}, \cite{braams:babel}, \cite{clark:pct}, \cite{herlihy:methodology}, \cite{Lamport:LaTeX}, \cite{salas:calculus}
\section{Conclusion}

\bibliographystyle{abbrv}
\bibliography{bib}

\appendix
\section{Proof of Correctness}


\setcounter{lemma}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}




\begin{definition}[Data types]
A \textbf{Block} $b$ is an object which
contains an array $b.array$, which itself can contain elements, $e \in Elem$,
where \textbf{Elem} represents the type of $e$ and can be any countable set. A
given block $b$ additionally contains an index $b.index$ which represents an
index location in $b.array$, a unique index identifying the array
$b.blockIndex$, and $b.next$, a reference to a successor block $c$ where
$c.blockIndex = b.blockIndex + 1$. A \textbf{Terminal} $term$ is a sentinel
object, which contains an integer $term.sealed \in \{-1\} \cup \mathbb{N}_0$, and
$term.callbacks$, a set of functions $f \in Elem \Rightarrow Unit$.

We define the following functions:

\begin{equation*}
following(b: Block) = 
\begin{cases}
\emptyset & \text{if b.next = null,}
\\
b.next \cup following(b.next) & \text{otherwise}
\end{cases}
\end{equation*}

\begin{equation*}
reachable(b: Block) = \{ b \} \cup following(b)
\end{equation*}

\begin{equation*}
last(b: Block) = b' : b' \in reachable(b) \wedge b'.next = null
\end{equation*}

\begin{equation*}
size(b: Block) = | \{ x : x \in b.array \wedge x \in Elem \} |
\end{equation*}

Based on them we define the following relation:

\begin{equation*}
reachable(b, c) \Leftrightarrow c \in reachable(b)
\end{equation*}
\end{definition}


\begin{definition}[FlowPool]
A \textbf{FlowPool} $pool$ is an object that
has a reference $pool.start$, to the first block $b_0$ (with $b_0.blockIndex=0$), 
as well as a reference $pool.current$.
We sometimes refer to these just as $start$ and $current$, respectively.

A \textbf{scheduled callback invocation} is a pair $(f, e)$ of a function
$f \in Elem => Unit$ and an element $e \in Elem$.
The programming construct that adds such a pair to the set of
$futures$ is \verb=future { f(e) }=.

The \textbf{FlowPool state} is defined as a pair of the directed graph of
objects transitively reachable from the reference $start$ and the set
of scheduled callback invocations called $futures$.

A \textbf{state changing} or \textbf{destructive} instruction is any
atomic write or CAS instruction that changes the FlowPool state.

We say that the FlowPool \textbf{has an element} $e$ at some time
$t_0$ if and only if the relation $hasElem(start, e)$ holds.

\begin{equation*}
hasElem(start, e) \Leftrightarrow \exists b \in reachable(start), e
\in b.array
\end{equation*}

We say that the FlowPool \textbf{has a callback} $f$ at some time
$t_0$ if and only if the relation $hasCallback(start, f)$ holds.

\begin{equation*}
hasCallback(start, f) \Leftrightarrow \forall b = last(start), b.array
= x^P \cdot t \cdot y^N, x \in Elem, t = Terminal(seal, callbacks), f
\in callbacks
\end{equation*}

We say that a callback $f$ in a FlowPool \textbf{will be called} for
the element $e$ at some time $t_0$ if and only if the relation
$willBeCalled(start, e, f)$ holds.

\begin{equation*}
willBeCalled(start, e, f) \Leftrightarrow \exists t_1, \forall t >
t_1, (f, e) \in futures
\end{equation*}

We say that the FlowPool is \textbf{sealed} at the size $s$ at some
$t_0$ if and only if the relation $sealedAt(start, s)$ holds.

\begin{equation*}
sealedAt(start, s) \Leftrightarrow s \neq -1 \wedge \forall b = last(start), b.array
= x^P \cdot t \cdot y^N, x \in Elem, t = Terminal(s, callbacks)
\end{equation*}

\textbf{FlowPool operations} are \verb=append=, \verb=foreach= and
\verb=seal=, and are defined by pseudocodes in figures ...
\end{definition}


\begin{definition}[Invariants]
We define the following invariants for the \textbf{FlowPool}:
\begin{description}
\item[INV1] $start = b: Block, b \neq null, current \in reachable(start)$
\item[INV2] $\forall b \in reachable(start), b \not \in following(b)$
\item[INV3] $\forall b \in reachable(start), b \neq last(start) \Rightarrow size(b) = LASTELEMPOS \wedge b.array(BLOCKSIZE - 1) \in Terminal$
\item[INV4]
$\forall b = last(start), b.array = p \cdot c \cdot n$, where:

$p = X^P, c = c_1 \cdot c_2, n = null^N$

$x \in Elem, c_1 \in Terminal, c_2 \in \{null\} \cup Terminal$

$P + N + 2 = BLOCKSIZE$
\item[INV5] $\forall b \in reachable(start), b.index > 0 \Rightarrow b.array(b.index - 1) \in Elem$
\end{description}
\end{definition}


\begin{definition}[Validity]
A FlowPool state $\mathbb{S}$ is \textbf{valid} if and only if the invariants [INV1-5] hold for that state.
\end{definition}


\begin{definition}[Abstract pool]
An \textbf{abstract pool} $\mathbb{P}$ is a function from time $t$ to a tuple $(elems, callbacks, seal)$ such that:
\begin{description}
\item $seal \in \{ -1 \} \cup \mathbb{N}_0$
\item $callbacks \subset \{ (f: Elem => Unit, called) \}$
\item $called \subseteq elems \subseteq Elem$
\end{description}
We say that an abstract pool $\mathbb{P}$ \textbf{is in state}
$\mathbb{A} = (elems, callbacks, seal)$ at time $t$ if and only if $\mathbb{P}(t) = (elems, callbacks, seal)$.
\end{definition}


\begin{definition}[Abstract pool operations]
We say that an \textbf{abstract pool operation} $op$ applied to an
abstract pool $\mathbb{P}$ in abstract state $\mathbb{A}_0 = (elems_0, callbacks_0, seal_0)$ 
at some time $t$ \textbf{changes} the abstract state of the abstract pool to $\mathbb{A} = (elems, callbacks, seal)$ 
if $\exists t_0, \forall \tau, t_0 < \tau < t, \mathbb{P}(\tau) = \mathbb{A}_0$
and $\mathbb{P}(t) = \mathbb{A}$.
We denote this as $\mathb{A} = op(\mathbb{A}_0)$.

Abstract pool operation $foreach(f)$ changes the abstract state at $t_0$ from $(elems, callbacks, seal)$ 
to $(elems, (f, \emptyset) \cup callbacks, seal)$. Furthermore:

$\exists t_1 \geq t_0, \forall t_2 > t_1, \mathbb{P}(t_2) = (elems_2,
callbacks_2, seal_2) \wedge \forall (f, called_2) \in callbacks_2,
elems \subseteq called_2 \subseteq elems_2$

Abstract pool operation $append(e)$ changes the abstract state at $t_0$ from
$(elems, callbacks, seal)$ to $(\{e\} \cup elems, callbacks, seal)$. Furthermore:

$\exists t_1 \geq t_0, \forall t_2 > t_1, \mathbb{P}(t_2) = (elems_2,
callbacks_2, seal_2) \wedge \forall (f, called_2) \in callbacks_2,
(f, called) \in callbacks \Rightarrow elem \in called_2$

Abstract pool operation $seal(s)$ changes the abstract state at $t_0$ from
$(elems, callbacks, seal)$ to $(elems, callbacks, s)$, assuming
that $seal \in \{-1\} \cup \{s\}$ and $s \in \mathbb{N}_0$, and
$|elems| \leq s$.
\end{definition}


\begin{definition}[Consistency]
A FlowPool state $\mathbb{S}$ is \textbf{consistent} with an abstract pool 
$\mathbb{P} = (elems, callbacks, seal)$ at $t_0$ if and only if $\mathbb{S}$ 
is a valid state and:
\begin{description}
\item $\forall e \in Elem, hasElem(start, e) \Leftrightarrow e \in elems$
\item $\forall f \in Elem => Unit, hasCallback(start, f) \Leftrightarrow f \in callbacks$
\item $\forall f \in Elem => Unit, \forall e \in Elem, willBeCalled(start, e, f) \Leftrightarrow \exists t_1 \geq t_0, \mathbb{P}(t_1) = (elems_1, (f, called_1) \cup callbacks_1, seal_1), elems \subseteq called_1$
\item $\forall s \in \mathbb{N}_0, sealedAt(start, s) \Leftrightarrow s = seal$
\end{description}

A FlowPool operation $op$ is \textbf{consistent} with the corresponding 
abstract state operation $op'$ if and only if $\mathbb{S'}=op(\mathbb{S})$ is consistent 
with an abstract state $\mathbb{A'}=op'(\mathbb{A})$.

A \textbf{consistency change} 
is a change from state $\mathbb{S}$ to state $\mathbb{S'}$ such that $\mathbb{S}$ 
is consistent with an abstract state $\mathbb{A}$ and $\mathbb{S'}$ is consistent 
with an abstract set $\mathbb{A'}$, where $\mathbb{A} \neq \mathbb{A'}$.

%A FlowPool operation $op$ completing at some time $t_0$ is \textbf{consistent} with an abstract pool operation $op'$ if 
%and only if $op$ changes the state of the FlowPool from $\mathbb{S}_1$ to $\mathbb{S}_2$, where $\mathbb{S}_1$ 
%and $\mathbb{S}_2$ are consistent with the abstract pool states $\mathbb{A}_1$ and $\mathbb{A}_2$, respectively, 
%and $op'$ changes the state of the abstract pool from $\mathbb{A}_1$ to $\mathbb{A}_2$.
\end{definition}


\begin{proposition}
Every valid state is consistent with some abstract pool.
\end{proposition}


\begin{theorem}[Safety]
FlowPool operation \verb=create= creates a new FlowPool consistent with the abstract pool 
$\mathbb{P} = (\emptyset, \emptyset, \emptyset)$. FlowPool operations \verb=foreach=, \verb=append= 
and \verb=seal= are consistent with the abstract pool semantics.
\end{theorem}


\begin{lemma}[End of life]\label{lemma-end-of-life}
For all blocks $b \in reachable(start)$, if value $v \in Elem$ is
written to $b.array$ at some position $idx$ at some time $t_0$, then
$\forall t > t_0, b.array(idx) = v$.
\end{lemma}

\begin{proof}
The CAS in line \ref{cas_append} is the only CAS which writes an
element.
No other CAS has a value of type $Elem$ as the expected value.
This means that once the CAS in line \ref{cas_append} writes a value
of type $Elem$, no other write can change it.
\end{proof}


\begin{corollary}\label{cor-end-of-life}
The end of life lemma implies that if all the values in $b.array$ are
of type $Elem$ at $t_0$, then $\forall t > t_0$ there is no write to $b.array$.
\end{corollary}


\begin{lemma}[Valid hint]\label{lemma-valid-hint}
For all blocks $b \in reachable(start)$, if $b.index > 0$ at some time $t_0$, then
$b.array(b.index - 1) \in Elem$ at time $t_0$.
\end{lemma}

\begin{proof}
Observe every write to $b.index$ -- they are all unconditional.
However, at every such write occurring at some time $t_1$ that writes
some value $idx$ we know that some previous value at $b.array$ entry $idx - 1$
at some time $t_0 < t_1$ was of type $Elem$.
Hence, from lemma \ref{lemma-end-of-life} it follows that
$\forall t \geq t_1, b.array(idx - 1) \in Elem$.
\end{proof}


\begin{corollary}[Compactness]\label{cor-compactness}
For all blocks $b \in reachable(start)$, if for some $idx$
$b.array(idx) \in Elem$ at time $t_0$ then $b.array(idx - 1) \in Elem$
at time $t_0$. This follows
directly from the lemmas \ref{lemma-end-of-life} and
\ref{lemma-valid-hint}, and the fact that the CAS in line
\ref{cas_append} only writes to array entries $idx$ for which it
previously read the value from $b.index$.
\end{corollary}


\begin{definition}[Transition]
If for a function $f(t)$ there exist times $t_0$ and $t_1$ such that
$\forall t, t_0 < t < t_1, f(t) = v_0$ and $f(t_1) = v_1$, then we say
that the function $f$ goes through a \textbf{transition} at $t_1$. We denote this as:

$f: v_0 \stackrel{t_1}{\rightarrow} v_1$

Or, if we don't care about the exact time $t_1$, simply as:

$f: v_0 \rightarrow v_1$
\end{definition}


\begin{definition}[Monotonicity]
A function of time $f(t)$ is said to be \textbf{monotonic}, if every value in its string of transitions occurs only once.
\end{definition}


\begin{lemma}[Freshness]\label{lemma-freshness}
For all blocks $b \in reachable(start)$, and for all $x \in b.array$,
function $x$ is monotonic.
\end{lemma}

\begin{proof}
CAS instruction in line \ref{cas_append} writes a value of type
$Elem$. No CAS instruction has a value of type $Elem$ as the expected value.

Trivial analysis of CAS instructions in lines \ref{cas_seal} and
\ref{cas_callback}, shows that their expected values are of type
$Terminal$. Their new values are always freshly allocated.

The more difficult part is to show that CAS instruction in line
\ref{cas_propagate} respects the statement of the lemma.

Since the CAS instructions in lines \ref{cas_seal} and
\ref{cas_callback} are preceeded by a read of $idx = b.index$,
from lemma \ref{lemma-valid-hint} it follows that $b.array(idx - 1)$ 
contains a value of type $Elem$.
These are also the only CAS instructions which replace a $Terminal$
value with another $Terminal$ value. The new value is always unique, as
shown above.

So the only potential CAS to write a non-fresh value to $idx + 1$ is the CAS
in line \ref{cas_propagate}.

A successful CAS in line \ref{cas_propagate} overwrites a value $cb_0$ at $idx + 1$
read in line \ref{read_next} at $t_0$ with a new value $cb_2$ at time $t_2$. Value $cb_2$ was
read in line \ref{read_current} at $t_1$ from the entry $idx$. The
string of transitions of values at $idx$ is composed of unique values
at least since $t_1$ (by lemma \ref{lemma-end-of-life}), since there is
a value of type $Elem$ at the index $idx - 1$.

The conclusion above ensures that the values read in line \ref{read_current}
to be subsequently used as new values for the CAS in line \ref{cas_propagate}
form a monotonic function $f(t) = b.array(idx) \text{ at } t$.

Now assume that a thread T1 successfully overwrites $cb_0$
via CAS in line \ref{cas_propagate} at $idx + 1$ at time $t_2$ 
to a value $cb_2$ read from $idx$ at $t_1$, and that another thread T2 
is the \textbf{first} thread (since the FlowPool was created) to subsequently successfully
complete the CAS in line \ref{cas_propagate} at $idx + 1$ at time
$t_{prev2} > t_2$ with some value $cb_{prev2}$ which was at $idx + 1$ at some time
$t < t_0$.

That would mean that $b.array(idx + 1)$ does not change during $\langle t_0, t_2 \rangle$,
since T2 was the first thread the write a non-fresh value to $idx + 1$, and any
other write would cause the CAS in line \ref{cas_propagate} by T1 to fail.
%That means that $t_0 > t_{prev0}$, otherwise the CAS in line \ref{cas_propagate}
%by T2 would have failed.

Also, that would mean that the thread T2 read the value
$cb_{prev2}$ in line \ref{read_current} at some time $t_{prev1} < t_1$
and successfully completed the CAS at time $t_{prev2} > t_2$. If the
CAS was successful, then the read in line \ref{read_next} by T2
occured at $t_{prev0} < t_{prev1} < t_1$. Since we assumed that T2 is the
first thread to write a value $cb_{prev2}$ to $idx + 1$ at time $t_{prev2}$
which was previously in $idx + 1$ at some time $t < t_0$, then the CAS
in line \ref{cas_propagate} at time $t_{prev2}$ could not have succeeded,
since its expected value is $cb_{prev0}$ read at some time $t_{prev0}$, and
we know that the value at $idx + 1$ was changed at least once in $\langle t_{prev0}, t_{prev2} \rangle$
because of the write of a fresh value by thread T1 at $t_2 \in \langle t_{prev0}, t_{prev2} \rangle$.
This value is known to be fresh because $b.array(idx)$ is a monotonic
function at least since $t_{prev1}$, and the read of the new value
written by T1 occurred at $t_1 > t_{prev1}$.
We also know that there is no other thread T3 to write the value
 $cb_{prev0}$ during $\langle t_{prev0}, t_{prev2} \rangle$
back to $idx + 1$, since we assumed that T2 is the first to write
a non-fresh value at that position.

Hence, a contradiction shows that there is no thread T2 which is the \textbf{first}
to write a non-fresh value via CAS in line \ref{cas_propagate} at $idx + 1$
for any $idx$, so there is no thread that writes a non-fresh value at all.
\end{proof}


\begin{lemma}[Lifecycle]\label{lemma-lifecycle}
For all blocks $b \in reachable(start)$, and for all $x \in b.array$, function $x$ goes through and only through the prefix of the following transitions:

$null \rightarrow cb_1 \rightarrow \dots \rightarrow cb_n \rightarrow elem$, where:

$cb_i \in Terminal, i \neq j \Rightarrow cb_i \neq cb_j, elem \in Elem$
\end{lemma}

\begin{proof}
First of all, it is obvious from the code that each block that becomes
an element of $reachable(start)$ at some time $t_0$ has the value of
all $x \in b.array$ set to $null$.

Next, we inspect all the CAS instructions that operate on entries of
$b.array$.

The CAS in line \ref{cas_append} has a value $curobj \in Terminal$ as
an expected value and writes an $elem \in Elem$.
This means that the only transition that this CAS
can cause is of type $cb_i \in Terminal \rightarrow elem \in Elem$.

We will now prove that the CAS in line \ref{cas_propagate} at time $t_2$ is successful if and
only if the entry at $idx + 1$ is $null$ or $nextobj \in
Terminal$.
We know that the entry at $idx + 1$ does not change $\forall t, t_0 < t < t_2$,
where $t_0$ is the read in line \ref{read_next},
because of lemma \ref{lemma-freshness} and the fact that CAS in line \ref{cas_propagate} is assumed to be successful.
We know that during the read in line \ref{read_current} at time $t_1$,
such that $t_0 < t_1 < t_2$, the entry at $idx$ was $curobj \in
Terminal$, by trivial analysis of the \verb=check= procedure.
It follows from corollary \ref{cor-compactness} that the array entry $idx
+ 1$ is not of type $Elem$ at time $t_1$, otherwise array entry $idx$
would have to be of type $Elem$.
Finally, we know that the entry at $idx + 1$ has the same value during
the interval $\langle t_1, t_2 \rangle$, so its value is not $Elem$ at $t_2$.

The above reasoning shows that the CAS in line \ref{cas_propagate}
always overwrites a one value of type $Terminal$ (or $null$) with
another value of type $Terminal$.
We have shown in lemma \ref{lemma-freshness} that it never
overwrites the value $cb_0$ with a value $cb_2$ that was at
$b.array(idx)$ at an earlier time.

Finally, note that the statement for CAS instructions in lines \ref{cas_seal} and
\ref{cas_callback} also follows directly from the proof for lemma \ref{lemma-freshness}.
\end{proof}


\begin{lemma}[Subsequence]\label{lemma-subsequence}
Assume that for some block $b \in reachable(start)$ the transitions of
$b.array(idx)$ are:

\begin{equation*}
b.array(idx): null \rightarrow cb_1 \rightarrow \cdots \rightarrow
cb_n \stackrel{t_0}{\rightarrow} elem: Elem
\end{equation*}

Assume that the transitions of $b.array(idx + 1)$ up to time $t_0$ are:

\begin{equation*}
b.array(idx + 1): null \rightarrow cb_1' \rightarrow \cdots
\rightarrow cb_m'
\end{equation*}

The string of transitions $null \rightarrow cb_1' \rightarrow \cdots
\rightarrow cb_m'$ is a subsequence of $null \rightarrow cb_1
\rightarrow \cdots \rightarrow cb_n$.
\end{lemma}

\begin{proof}
Note that all the values written to $idx + 1$ before $t_0$ by CAS in line \ref{cas_propagate} were
previously read from $idx$ in line \ref{read_current}.
This means that the set of values occurring in $b.array(idx + 1)$
before $t_0$ is a subset of the set of values in $b.array(idx)$.
We have to prove that it is actually a subsequence.

Assume that there exist two values $cb_1$ and $cb_2$ read by threads T1 and T2
in line \ref{read_current} at times $t_1$ and $t_2 > t_1$, respectively.
Assume that these values are written to $idx + 1$ by threads T1 and T2
in line \ref{cas_propagate} in the opposite order, that is at times
$t_{cas1}$ and $t_{cas2} < t_{cas1}$, respectively.
That would mean that the CAS by thread T1 would have to fail, since its expected
value $cb_0$ has changed between the time it was read in line \ref{read_next} and
the $t_{cas1}$ at least once to a different value, and it could not have been
changed back to $cb_0$ as we know from the lemma
\ref{lemma-freshness}.

Notice that we have actually prooved a stronger result above.
We have also shown that the string of values
written at $idx + 1$ by CAS in line \ref{cas_propagate} successfully is a subsequence
of \textbf{all} the transitions of values at $idx$ (not just until $t_0$).
\end{proof}


\begin{lemma}[Valid writes]\label{lemma-valid}
Given a FlowPool in a valid state, all writes in all operations produce a FlowPool in a valid state.
\end{lemma}

\begin{proof}
A new FlowPool is trivially in a valid state.

Otherwise, assume that the FlowPool is in a valid state
$\mathbb{S}$.
In the rest of the proof, whenever some invariant is trivially
unaffected by a write, we omit mentioning it.
We start by noting that we already prooved the claim
for atomic writes in lines \ref{write_append}, \ref{write_advance} and
\ref{write_seal} (which only affect [INV5]) in lemma
\ref{lemma-valid-hint}.
We proceed by analyzing each atomic CAS instruction.

CAS in line \ref{cas_expand} at time $t_1$ maintains the invariant
[INV1].
This is because its expected value is always
$null$, which ensures that the lifecycle of $b.next$ is $null
\rightarrow b': Block$, meaning that the function $reachable(start)$
returns a monotonically growing set.
So if $current \in reachable(start)$ at $t_0$, then this also holds at
$t_1 > t_0$.
It also maintains [INV2] because the new value $nb$ is always fresh,
so $\forall b, b \not \in following(b)$.
Finally, it maintains [INV3] because it is preceeded with a bounds
check and we know from corollary \ref{cor-compactness} and the
lemma \ref{lemma-end-of-life} that all the values in $b.array(idx),
idx < LASTELEMPOS$ must be of type $Elem$.

CAS in line \ref{cas_block} at time $t_1$ maintains the
invariant [INV1], since the new value for the $current \neq null$ was read from
$b.next$ at $t_0 < t_1$ when the invariant was assumed to hold, and
it is still there a $t_1$, as shown before.

For CAS instructions in lines \ref{cas_append}, \ref{cas_callback} and
\ref{cas_seal} that write to index $idx$ we know from lemma
\ref{lemma-valid-hint} that the value at $idx - 1$ is of type $Elem$.
This immediately shows that CAS instructions in lines
\ref{cas_callback} and \ref{cas_seal} maintain [INV3] and [INV4].

For CAS in line \ref{cas_append} we additionally know that it must
have been preceeded by a successful CAS in line \ref{cas_propagate}
which previously wrote a $Terminal$ value to $idx + 1$. From lemma
\ref{lemma-lifecycle} we know that $idx + 1$ is still $Terminal$ when
the CAS in line \ref{cas_append} occurs, hence [INV4] is kept.

Finally, CAS in line \ref{cas_propagate} succeeds only if the value at
$idx + 1$ is of type $Terminal$, as shown before in lemma
\ref{lemma-lifecycle}.
By the same lemma, the value at $idx$ is either
$Terminal$ or $Elem$ at that point, since $idx - 1$ is known to be
$Elem$ by lemma \ref{lemma-valid-hint}.
This means that [INV4] is kept.
\end{proof}


\begin{lemma}[Housekeeping]\label{lemma-housekeeping}
Given a FlowPool in state $\mathbb{S}$ consistent with some abstract
pool state $\mathbb{A}$, CAS instructions in lines \ref{cas_propagate}, \ref{cas_expand} and
\ref{cas_block} do not change the abstract pool state $\mathbb{A}$.
\end{lemma}

\begin{proof}
Since none of the relations $hasElem$, $hasCallback$, $willBeCalled$ and $sealedAt$
are defined by the value of $current$ CAS in line \ref{cas_block}
does not change them, hence it does not change the abstract pool
state.

No CAS changes the set of scheduled futures, nor is
succeeded by a \verb=future= construct so it does not affect
the $willBeCalled$ relation.

It is easy to see that the CAS in line \ref{cas_expand} does not remove any elements, nor make
any additional elements reachable, since the new block $nb$ which
becomes reachable does not contain any elements at that time.
Hence the $hasElem$ relation is not affected.
It does change the value $last(start)$ to $nb$, but since $nb.array =
t \cdot null^{BLOCKSIZE - 1}$, where $t \in Terminal$ was previously
the last non-null element in $b.array$, it does changes neither the
$sealedAt$ nor the $hasCallback$ relation.

The CAS in line \ref{cas_propagate} does not make some new element reachable,
hence the $hasElem$ relation is preserved.

Note now that this CAS does not change the relations $hasCallback$
and $sealedAt$ as long as there is a value of type $Terminal$ at the
preceeding entry $idx$.
We claim that if the CAS succeeds at $t_2$, then 
either the value at $idx$ is of type $Terminal$ (trivially) or the CAS
did not change the value at $idx + 1$.
In other words, if the value at $idx$ at time $t_2$ is of type $Elem$,
then the write by CAS in line \ref{cas_propagate} does not change
the value at $idx + 1$ at $t_2$.
This was, in fact, already shown in the proof of lemma \ref{lemma-subsequence}.

The argument above proves directly that relations $hasCallback$
and $sealedAt$ are not changed by the CAS in line \ref{cas_propagate}.
\end{proof}


\begin{lemma}[Append correctness]\label{lemma-append}
Given a FlowPool in state $\mathbb{S}$ consistent with some abstract pool state $\mathbb{A}$, 
a successful CAS in line \ref{cas_append} at some time $t_0$ changes the state of the FlowPool 
to $\mathbb{S}_0$ consistent with an abstract pool state $\mathbb{A}_0$, such that:

$\mathbb{A} = (elems, callbacks, seal)$

$\mathbb{A}_0 = (\{elem\} \cup elems, callbacks, seal)$

Furthermore, given a fair scheduler, there exists a time $t_1 > t_0$ at which the FlowPool 
is consistent with an abstract pool in state $\mathbb{A}_1$, such that:

$\mathbb{A}_1 = (elems_1, callbacks_1, seal_1)$, where:

$\forall (f, called_1) \in callbacks_1, (f, called) \in callbacks \Rightarrow elem \in called_1$
\end{lemma}

\begin{proof}
Assume that the CAS in line \ref{cas_append} succeeds at some time
$t_3$, the CAS in line \ref{cas_propagate} succeeds at some time $t_2 <
t_3$, the read in line \ref{read_current} occurs at some time $t_1 <
t_2$ and the read in line \ref{read_current} occurs at some time $t_0
< t_1$.

It is easy to see from the invariants, $check$ procedure and the
corollary \ref{cor-end-of-life} that the CAS in line \ref{cas_append} can only
occur if $b = last(start)$.

We claim that for the block $b \in reachable(start)$ such that $b = last(b)$ the
following holds at $t_2$:

$b.array = elem^N \cdot cb_1 \cdot cb_2 \cdot null^{BLOCKSIZE - N - 2}$

where $cb_1 = cb_2$, since there was no write to $idx$ after $cb_1$, otherwise the
CAS in line \ref{cas_append} at $t_3$ would not have been successful
(by lemma \ref{lemma-freshness}).

Furthermore, $cb_1 = cb_2$ at $t_3$, as shown in the lemma
\ref{lemma-subsequence}. Due to the same lemma, the entries of
$b.array$ stay the same until $t_3$, otherwise the CAS in line
\ref{cas_append} would not have been successful.
After the successful CAS at $t_3$, we have:

$b.array = elem^N \cdot e \cdot cb_1 \cdot null^{BLOCKSIZE - N - 2}$

where $e: Elem$ is the newly appended element -- at $t_3$ the
relation $hasElem(start, e)$ holds, and $sealedAt(start, s)$ and
$hasCallback(start, f)$ did not change between $t_2$ and $t_3$.

It remains to be shown that $willBeCalled(start, e, f)$ holds at $t_3$.
Given a fair scheduler, within a finite number of steps the
future store will contain a request for an asynchronous computation
that invokes $f$ on $e$. The fair scheduler ensures that the future is
scheduled within a finite number of steps.
\end{proof}


\begin{lemma}[Foreach correctness]\label{lemma-foreach}
Given a FlowPool in state $\mathbb{S}$ consistent with some abstract pool state $\mathbb{A}$, 
a successful CAS in line \ref{cas_callback} at some time $t_0$ changes the state of the FlowPool 
to $\mathbb{S}_0$ consistent with an abstract pool state $\mathbb{A}_0$, such that:

$\mathbb{A} = (elems, callbacks, seal)$

$\mathbb{A}_0 = (elems, (f, \emptyset) \cup callbacks, seal)$

Furthermore, given a fair scheduler, there exists a time $t_1 \geq t_0$ at which the FlowPool 
is consistent with an abstract pool in state $\mathbb{A}_1$, such that:

$\mathbb{A}_1 = (elems_1, callbacks_1, seal_1)$, where:

$elems \subseteq elems_1$

$\forall (f, called_1) \in callbacks_1, elems \subseteq called_1$
\end{lemma}

\begin{proof}
From lemma \ref{lemma-freshness} and the assumption that the CAS is
successful we know that the value at $b.array(idx)$ has not changed
between the read in line \ref{read_callbacks} and the CAS in line
\ref{cas_callback}.
From lemma \ref{lemma-valid-hint} we know that the value at $idx - 1$
was of type $Elem$ since $b.index$ was read.
This means that neither $hasElem(start, e)$ nor $sealedAt$ have changed after the CAS.
Since after the CAS there is a $Terminal$ with an additional function $f$ at $idx$,
the $hasCallback(start, f)$ holds after the CAS.
Finally, the $willBeCalled(start, e, f)$ holds for all elements $e$
for which the $hasElem(e)$ holds, since the CAS has been preceeded by
a call $f(e)$ in line \ref{call_callback} for each element. The lemma
\ref{lemma-end-of-life} ensures that for each element $f$ was called
for stays in the pool indefinitely (i.e. is not removed).

Trivially, the time $t_1$ from the statement of the lemma is such that $t_1 = t_0$.
\end{proof}


\begin{lemma}[Seal correctness]\label{lemma-seal}
Given a FlowPool in state $\mathbb{S}$ consistent with some abstract pool state $\mathbb{A}$, 
a successful CAS in line \ref{cas_callback} at some time $t_0$ changes the state of the FlowPool 
to $\mathbb{S}_0$ consistent with an abstract pool state $\mathbb{A}_0$, such that:

$\mathbb{A} = (elems, callbacks, seal)$, where $seal \in \{ \emptyset, \{ s \} \}$

$\mathbb{A}_0 = (elems, callbacks, \{ s \})$
\end{lemma}

\begin{proof}
Similar to the proof of lemma \ref{lemma-foreach}.
\end{proof}


\begin{definition}[Obstruction-freedom]
Given a FlowPool in a valid state, an operation $op$ is
\textbf{obstruction-free} if and only if a thread T executing the
operation $op$ completes within a finite number of steps given that
no other thread was executing the operation $op$ since T started executing it.

We say that thread T executes the operation $op$ \textbf{in isolation}.
\end{definition}


\begin{lemma}[Obstruction-free operations]\label{lemma-obstruction-free}
The FlowPool operations are obstruction-free.
\end{lemma}

\begin{proof}
By trivial sequential code analysis supported by the fact that the
invariants (especially [INV2]) hold in a valid state.
\end{proof}

\begin{proof}[Safety]
From lemmas \ref{lemma-housekeeping}, \ref{lemma-append}, \ref{lemma-foreach} and
\ref{lemma-seal} directly, along with the fact that all operations
executing in  isolation complete after a finite number of steps by lemma \ref{lemma-obstruction-free}.
\end{proof}

% \textbf{Definition 2} (FlowPool). typically pointing to some
% subsequent block $b_n$ where $b_n=b_0$ or where $b_n$ is reachable from $b_0$
% following $next$ references. Initially, $pool.current = pool.start$. The pool
% \textbf{state} $\mathbb{S}$ is defined as the sequence of blocks reachable
% from $pool.start$ by following $next$ references within blocks.
% A \textbf{state changing} instruction is any atomic write or CAS instruction
% that changes an object that can be accessed from $pool.start$.

% \textbf{Definition 3} (Invariants).

% \textbf{INV1} let $b: Block$ such that $reachable(start,b)\wedge b.next=null$. 
% $\exists i$ such that $b.array(i) = Terminal \wedge \forall~i<j \leq LASTELEMPOS$, 
% $b.array(j) = null$

% \textbf{Definition 4} (Abstract state). An \textbf{abstract state} $\mathbb{A}$ 
% is a tuple $(elems,sealed,callbacks)$ such that 
% $\mathbb{A} \in \{(elems,sealed,callbacks)~|~elems \subset Elem,~sealed \in \{-1\} \cup \mathbb{N}_0,~callbacks\subset Elem \Rightarrow Unit\}$ 
% Abstract state operations on some abstract state $\mathbb{A}$ are $append(\mathbb{A},e) = \mathbb{A'}$ 
% where $\mathbb{A'} = (elems \cup \{e\}, sealed, callbacks)$ if $\mathbb{A}=(elems, sealed, callbacks)$, 
% $seal(\mathbb{A},sealSize) = \mathbb{A''}$ where $\mathbb{A''} = (elems, sealSize, callbacks)~:~sealSize \in \mathbb{N}_0$ 
% if $\mathbb{A}=(elems, sealed, callbacks)$ and $sealed=-1$, the unsealed state, or 
% $sealed=sealSize$ already, and 
% $doForAll(\mathbb{A},fun)=\mathbb{A'''}$ where $\mathbb{A'''}=(elems, sealed, callbacks\cup \{fun\})$.

% \textbf{Definition 5} (Consistency). A FlowPool state $\mathbb{S}$ of $pool$ 
% with starting block $pool.start$ is consistent with an abstract state 
% $\mathbb{A}=(elems,sealed)$ iff some element $e \in elems \Leftrightarrow \exists b,i~:~reachable(pool.start,b)~\wedge~b.array(i)=e,$ 
% and $\exists c,j~:~c.array(j) \in Terminal~\wedge~c.array(j).sealed=sealed~\wedge~reachable(pool.start, c)$. 

\textbf{Definition 6} (Lock-freedom). In a scenario where some finite number
of threads are executing a concurrent operation, that concurrent operation is
\textit{lock-free} if and only if that concurrent operation is completed after
a finite number of steps by some thread.

%==============================
%==============================
% 
% THEOREM 1
%
%==============================
%==============================

\textbf{Theorem 1} (Lock-freedom). \textit{FlowPool operations append, seal,
and doForAll are lock-free}.

We begin by first proving that there are a finite number of execution steps
before a consistency change occurs.

By Lemma 5, after invoking $append$, a consistency change occurs after a
finite number of steps. Likewise, by Lemma 7, after invoking $seal$, a
consistency change occurs after a finite number of steps. And finally, by
Lemma 8, after invoking $doForAll$, a consistency change likewise occurs after
a finite number of steps.

By Lemma 10, this means a concurrent operation $append$, $seal$, or $doForAll$
will successfully complete. Therefore, by Definition 6, these operations are
lock-free.

% CASes that fail by being completed by other CASes signify progress. 

%==============================
% LEMMA 1
%==============================

\textbf{Lemma 1}. \textit{After invoking an operation $op$, if non-consistency
changing CAS operations $CAS1$, $CAS3$, or $CAS4$, in the pseudocode fail,
they must have already been successfully completed by another thread since
$op$ began}.

\textit{Proof}. Trivial inspection of the pseudocode reveals that since $CAS1$
makes up a check that precedes $CAS2$, and since $CAS2$ is the only operation
besides $CAS1$ which can change the expected value of $CAS1$, in the case of
a failure of $CAS1$, $CAS2$ (and thus $CAS1$) must have already successfully
completed or $CAS1$ must have already successfully completed by a different
thread since $op$ began executing.

Likewise, by trivial inspection $CAS3$ is the only CAS which can update the
$b.next$ reference, therefore in the case of a failure, some other thread must
have already successfully completed $CAS3$ since the beginning of $op$.

Like above, $CAS4$ is the only CAS which can change the $current$ reference,
therefore in the case of a failure, some other thread must have already
successfully completed $CAS4$ since $op$ began.
\qed

%==============================
% LEMMA 2 (EXPAND)
%==============================

\textbf{Lemma 2}. \textit{Invoking the $expand$ operation will execute a non-
consistency changing instruction after a finite number of steps. Moreover, it
is guaranteed that the $current$ reference is updated to point to a subsequent
block after a finite number of steps. Finally, $expand$ will return after a
finite number of steps.
% Furthermore, given a total number of blocks $numBlocks$ reachable in a
% FlowPool $pool$ before invoking $expand$ through $append$, the number of
% blocks $numBlocks'$ after some finite number of steps is guaranteed to satisfy
% $numBlocks'>numBlocks$
}.

% There are some issues with this one. Later on, we only actually need the
% point about the current reference, and also the point that expand returns
% after a finite number of steps. So we don't actually need the points about
% numBlocks. 
%
% Not guaranteed to create a new block, because invoking expand from seal goes
% straight to case 2 in if/else. Should remove that.
%
% So, right before calling expand, some other thread might update the next
% pointer, and then the only thing that remains to be done is to change
% current. Because expand checks whether next is null. If not null, then the
% only thing that it does is update current. So the wording in the 2 cases
% below needs to be cleaned up and some stuff needs to be removed.

\textit{Proof}. From inspection of the pseudocode, it is clear that the only
point at which $expand(b)$ can be invoked is under the condition that for some
block $b$, $b.index > LASTELEMPOS$, where $LASTELEMPOS$ is the maximum size
set aside for elements of type $Elem$ in any block. Given this, we will proceed 
by showing that a new block will be created with all related references 
$b.next$ and $current$ correctly set.

There are two conditions under which a non-consistency changing CAS
instruction will be carried out. 

\begin{itemize} 
\item \textbf{Case 1:} if $b.next=null$, a new block $nb$ will
be created and $CAS3$ will be executed. From Lemma 1, we know that $CAS3$
must complete successfully on some thread. Afterwards recursively calling 
$expand$ on the original block $b$.
\item \textbf{Case 2:} if $b.next \neq null$, $CAS4$ will be executed. 
Lemma 1 guarantees that $CAS4$ will update $current$ to refer to $b.next$, 
which we will show can only be a new block. Likewise, Lemma 1 has shown that 
$CAS3$ is the only state changing instruction that can initiate a state change 
at location $b.next$, therefore, since $CAS3$ takes place within Case 1,  
Case 2 can only be reachable after Case 1 has been executed successfully. Given 
that Case 1 always creates a new block, therefore, $b.next$ in this case, must 
always refer to a new block.
\end{itemize}

Therefore, since from Lemma 1 we know that both $CAS3$ and $CAS4$ can only
fail if already completed guaranteeing their finite completion, and since
$CAS3$ and $CAS4$ are the only state changing operations invoked through
$expand$, the $expand$ operation must complete in a finite number of steps.

Finally, since we saw in Case 2 that a new block is always created and related
references are always correctly set, that is both $b.next $ and $current$ are
correctly updated to refer to the new block, it follows that $numBlocks$
strictly increases after some finite number of steps.
\qed

%==============================
% LEMMA 3 (CAS2)
%==============================

\textbf{Lemma 3}. \textit{After invoking $append(elem)$, if $CAS2$ fails, then
some thread has successfully completed $CAS2$ or $CAS5$ (or likewise, $CAS6$)
after some finite number of steps}.

\textit{Proof}. First, we show that a thread attempting to complete $CAS2$
can't fail due to a different thread completing $CAS1$ so long as $seal$ has
not been invoked after completing the read of $currobj$. We address this
exception later on.

Since after $check$, the only condition under which $CAS1$, and by extension,
$CAS2$ can be executed is the situation where the current object $currobj$
with index location $idx$ is the $Terminal$ object, it follows that $CAS1$ can
only ever serve to duplicate this $Terminal$ object at location $idx+1$,
leaving at most two $Terminal$s in block refered to by $current$ momentarily
until $CAS2$ can be executed. By Lemma 1, since $CAS1$ is a non-consistency
changing instruction, it follows that any thread holding any element $elem'$
can execute this instruction without changing the expected value of $currobj$
in $CAS2$, as no new object is ever created and placed in location $idx$.
Therefore, $CAS2$ cannot fail due to $CAS1$, so long as $seal$ has not been
invoked by some other thread after the read of $currobj$.

This leaves only two scenarios in which consistency changing $CAS2$ can fail:

\begin{itemize}
\item \textbf{Case 1:} Another thread has already completed $CAS2$ with a 
different element $elem'$.
\item \textbf{Case 2:} Another thread completes an invocation to the $seal$ 
operation after the current thread completes the read of $currobj$. In this 
case, $CAS2$ can fail because $CAS5$ (or, likewise $CAS6$) might have 
completed before, in which case, it inserts a new $Terminal$ object $term$ 
into location $idx$ (in the case of a $seal$ invocation, 
$term.sealed\in\mathbb{N}_0$, or in the case of a $doForAll$ invocation, 
$\{term.callbacks\in$\}). 
\end{itemize}

We omit the proof and detailed discussion of $CAS6$ because it can be proven 
using the same steps as were taken for $CAS5$. 
\qed

%==============================
% LEMMA 4 (FINITE # STEPS BEFORE STATE CHANGE)
%==============================

\textbf{Lemma 4}. \textit{All operations with the exception of $append$,
$seal$, and $doForAll$ execute only a finite number of steps between each
state changing instruction}

\textit{Proof}. The \textbf{advance}, \textbf{check}, \textbf{totalElems},
\textbf{invokeCallbacks}, and \textbf{tryWriteSeal}  operations have a finite
number of execution steps, as they contain no recursive calls, loops, or other
possibility to restart.

While the \textbf{expand} operation contains a recursive call following a CAS
instruction, it was shown in Lemma 2 that an invocation of $expand$ is
guaranteed to execute a state changing instruction after a finite number of
steps. 
\qed

%==============================
% LEMMA 5 (APPEND: FINITE # STEPS BEFORE CONSISTENCY CHANGE)
%==============================

\textbf{Lemma 5}. \textit{After invoking $append(elem)$, a consistency
changing instruction will be completed after a finite number of steps.}

\textit{Proof}.
The \textbf{append} operation can be restarted in three cases. We show that in
each case, it's guaranteed to either complete in a finite number of steps,  or
leads to a state changing instruction:

\begin{itemize} 

\item \textbf{Case 1:} The call to $check$, a finite operation by Lemma 4,
returns $false$,  causing a call to $advance$, also a finite operation by
Lemma 4, followed by a recursive call to $append$ with the same  element
$elem$ which in turn once again calls $check$.

We show that after a finite number of steps, the $check$ will evaluate to
$true$, or some other thread will have completed a consistency changing
operation since the initial invocation of $append$. In the case where $check$
evaluates to $true$, Lemma 3 applies, as it guarantees that a consistency
changing CAS is completed after a finite number of steps.

When the call to the finite operation $check$ returns $false$, if the
subsequent $advance$ finds that a $Terminal$ object is at the current block
index $idx$, then the next invocation of $append$ will evaluate $check$ to
$true$. Otherwise, it must be the case that another thread has moved the
Terminal to a subsequent index since the initial invocation of append, which
is only possible using a consistency changing instruction.

Finally, if $advance$ finds that the element at $idx$ is an $Elem$, by Lemma
9, $b.index$ will be incremented after a finite number of steps. By $INV1$,
this can only happen a finite number of times until a $Terminal$ is found. In
the case that $expand$ is meanwhile invoked through $advance$, by Lemma 2 it's
guaranteed to complete state changing instructions $CAS3$ or $CAS4$ in a
finite number of steps. Otherwise, some other thread has moved the $Terminal$
to a subsequent index. However, this latter case is only possible by
successfully completing $CAS2$, a consistency changing instruction, after the
initial invocation of append.

\item \textbf{Case 2:} $CAS1$ fails, which we know from Lemma 1 means that it
must've already been completed by another thread, guaranteeing that $CAS2$
will be attempted. If $CAS2$ fails, by Lemma 3, after a finite number of
steps, a consistency changing instruction will be completed. If $CAS2$
succeeds, as a consistency changing instruction, consistency will have
clearly been changed.

\item \textbf{Case 3:} $CAS2$ fails, which, by Lemma 3, indicates that either
some other thread has already completed $CAS2$ with another element, or
another consistency changing instruction, $CAS5$ or $CAS6$ has successfully
completed.  

\end{itemize}

Therefore, $append$ itself as well as all other operations reachable via an
invocation of $append$ are guaranteed to have a finite number of steps between
\textit{consistency} changing instructions.

%==============================
% LEMMA 6 (CAS5)
%==============================

\textbf{Lemma 6}. \textit{After invoking $seal(size)$, if $CAS5$ fails, then
some thread has successfully completed $CAS5$ or $CAS2$ after some finite
number of steps}.

\textit{Proof}. Since $CAS1$ only duplicates an existing $Terminal$, it can
not be the cause for a failing $CAS5$. This leaves only two cases in which
$CAS5$ can fail:

\begin{itemize}
\item \textbf{Case 1:} Another thread has already completed $CAS5$.
\item \textbf{Case 2:} Another thread completes an invocation to the
$append(elem)$ operation after the current thread completes the read of
$currobj$. In this  case, $CAS5$ can fail because $CAS2$ might have  completed
before, in which case, it inserts a new $Elem$ object $elem$  into location
$idx$.

\end{itemize}

%==============================
% LEMMA 7 (SEAL: FINITE # STEPS BEFORE CONSISTENCY CHANGE)
%==============================

\textbf{Lemma 7}. \textit{After invoking $seal(size)$, a consistency changing
instruction will be completed after a finite number of steps, or the initial
invocation of $seal(size)$ completes.}

\textit{Proof}. The $seal$ operation can be restarted in two scenarios. 

\begin{itemize}

\item \textbf{Case 1:} The check $idx \leq LASTELEMPOS$ succeeds, indicating
that we are at a valid location in the current block $b$, but the object at
the current index location $idx$ is of type $Elem$, not $Terminal$, causing a
recursive call to $seal$ with the same size $size$.

In this case, we begin by showing that the atomic write of $idx+1$ to
$b.index$, required to iterate through the block $b$ for the recursive call to
$seal$, will be correctly incremented after a finite number of steps. 

Therefore, by both the guarantee that, in a finite number of steps, $b.index$
will eventually be correctly incremented as we saw in Lemma 9, as well as by
$INV1$ we know that the original invocation of $seal$ will correctly iterate
through $b$ until a $Terminal$ is found. Thus, we know that the call to
$tryWriteSeal$ will be invoked, and by both Lemma 4 and Lemma 5, we know that
either $tryWriteSeal$, will successfully complete in a finite number of steps,
in turn successfully completing $seal(size)$, or $CAS2$, another consistency
changing operation will successfully complete.

\item \textbf{Case 2:} The check $idx \leq LASTELEMPOS$ fails, indicating that
we must move on to the next block, causing first a call to $expand$ followed
by a recursive call to $seal$ with the same size $size$. 

We proceed by showing that after a finite number of steps, we must end up in
Case 1, which we have just showed itself completes in a finite number of
steps, or that a consistency change must've already occurred. 

By Lemma 2, we know that an invocation of $expand$ returns after a finite
number of steps, and $pool.current$ is updated to point to a subsequent block.

If we are in the recursive call to $seal$, and the $idx \leq LASTELEMPOS$
condition is $false$, trivally, a consistency changing operation must have
occurred, as, the only way for the condition to evaluate to $true$ is through
a consistency changing operation, in the case that a block has been created
during an invocation to $append$, for example.

% In the case, where after the invocation $expand$, but before the read of $b$
% ($READ(pool.current)$) in the recursive call to $seal$, we have that
% $current.index > LASTELEMPOS$, the a consistency change must have occurred.
% That is, another thread has already updated $pool.current$ by a different
% invocation to $expand$, which means that, in the meantime, an element has
% already been inserted via $append$. 

% Trivally, a consistency changing operation must have occurred, as, the only
% way for $current.index > LASTELEMPOS$ to evaluate to $true$ is if a block .

% % Thus, as the only way for the index to
% % change is via $WRITE1$ which occurs only if $CAS2$ succeeds, a consistency
% % changing instruction must have completed.

Otherwise, if we are in the recursive call to $seal$, and the $idx \leq
LASTELEMPOS$ condition evaluates to $true$, we enter Case 1, which we just
showed will successfully complete in a finite number of steps.

\end{itemize}


%==============================
% LEMMA 8 (DOFORALL: FINITE # STEPS BEFORE CONSISTENCY CHANGE)
%==============================

\textbf{Lemma 8}. \textit{After invoking $doForAll(fun)$, a consistency
changing instruction will be completed after a finite number of steps.}

We omit the proof for $doForAll$ since it proceeds in the exactly the same way
as does the proof for $seal$ in Lemma 7.

%==============================
% LEMMA 9, WRITE2 AND WRITE3
%==============================

\textbf{Lemma 9}. \textit{After updating $b.index$ using $WRITE2$ or $WRITE3$, 
$b.index$ is guaranteed to be incremented after a finite number of STEPS}.

\textit{Proof}. For some index, $idx$, both calls to $WRITE2$ and $WRITE3$
attempt to write $idx+1$ to $b.index$. In both cases, it's possible that
another thread could complete either $WRITE2$ or $WRITE3$, once again writing
$idx$ to $b.index$ after the current thread has completed, in effect
overwriting the current thread's write with $idx+1$. By inspection of the
pseudocode, both $WRITE2$ and $WRITE3$ will be repeated if $b.index$ has not
been incremented. However, since the number of threads operating on the
FlowPool is finite, $p$, we are guaranteed that in the worst case, this
scenario can repeat at most $p$ times, before a write correctly updates
$b.index$ with $idx+1$.
\qed

%==============================
% LEMMA 10
%==============================

\textbf{Lemma 10}. \textit{Assume some concurrent operation is started. If
some thread completes consistency changing CAS instruction, then some
concurrent operation is guaranteed to be completed}.

\textit{Proof}. 
% By Lemma X and Y, we know that consistency changing
% instructions $CAS2$, $CAS5$, and $CAS6$ are guaranteed to at some point
% complete.

By trival inspection of the pseudocode, if $CAS2$ successfully completes on
some thread, then that thread is guaranteed to complete the corresponding
invocation of $append$ in a finite number of steps.
% we don't say anything about WRITE1 here because it's guaranteed to succeed, so it's trivial.

Likewise by trivial inspection, if $CAS5$ successfully completes on some
thread, then by Lemma 4, $tryWriteSeal$ is guaranteed to complete in a finite
number of steps, and therefore, that thread is guaranteed to complete the
corresponding invocation of $seal$ in a finite number of steps.

The case for $CAS6$ is omitted since it follows the same steps as for the case
of $CAS5$

\pagebreak


\end{document}
